{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjUlXyQiWNwK",
        "outputId": "182660d4-7984-45ed-f731-f358ac853ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 10000 traces with 600 samples each...\n",
            "Output directory: /content/sca_outputs\n",
            "Using fixed key byte: 0x66\n",
            "Generating 10000 traces...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:01<00:00, 5787.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation complete!\n",
            "Traces shape: (10000, 600)\n",
            "Labels shape: (10000,)\n",
            "Unique labels (Hamming weights): [0 1 2 3 4 5 6 7 8]\n",
            "Label distribution: {np.int64(0): np.int64(35), np.int64(1): np.int64(294), np.int64(2): np.int64(1109), np.int64(3): np.int64(2128), np.int64(4): np.int64(2740), np.int64(5): np.int64(2227), np.int64(6): np.int64(1108), np.int64(7): np.int64(315), np.int64(8): np.int64(44)}\n",
            "\n",
            "Measured SNR: 22.13 dB\n",
            "\n",
            "Dataset saved to: /content/sca_outputs/dataset_small.pkl\n",
            "File size: 46.18 MB\n",
            "Example traces saved to: /content/sca_outputs/example_traces.pkl\n",
            "\n",
            "==================================================\n",
            "DATASET GENERATION COMPLETE\n",
            "==================================================\n",
            "Total traces: 10000\n",
            "Trace length: 600 samples\n",
            "Number of classes: 9 (Hamming weights 0-8)\n",
            "True key byte: 0x66\n",
            "\n",
            "You can now run the training script!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- CONFIG ----\n",
        "OUT_DIR = \"/content/sca_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Dataset parameters\n",
        "NUM_TRACES = 10000  # Increase for better accuracy\n",
        "TRACE_LENGTH = 600\n",
        "NUM_DEVICES = 5  # Simulate multiple devices for variability\n",
        "\n",
        "# Leakage parameters\n",
        "LEAK_CENTER = 300  # Where the leakage occurs in the trace\n",
        "LEAK_WIDTH = 40    # Width of the leakage kernel\n",
        "BASE_NOISE = 1.0   # Base noise level\n",
        "SNR = 3.0          # Signal-to-noise ratio (higher = easier)\n",
        "\n",
        "print(f\"Generating {NUM_TRACES} traces with {TRACE_LENGTH} samples each...\")\n",
        "print(f\"Output directory: {OUT_DIR}\")\n",
        "\n",
        "# ---- AES S-BOX ----\n",
        "AES_SBOX = np.array([\n",
        "    0x63,0x7c,0x77,0x7b,0xf2,0x6b,0x6f,0xc5,0x30,0x01,0x67,0x2b,0xfe,0xd7,0xab,0x76,\n",
        "    0xca,0x82,0xc9,0x7d,0xfa,0x59,0x47,0xf0,0xad,0xd4,0xa2,0xaf,0x9c,0xa4,0x72,0xc0,\n",
        "    0xb7,0xfd,0x93,0x26,0x36,0x3f,0xf7,0xcc,0x34,0xa5,0xe5,0xf1,0x71,0xd8,0x31,0x15,\n",
        "    0x04,0xc7,0x23,0xc3,0x18,0x96,0x05,0x9a,0x07,0x12,0x80,0xe2,0xeb,0x27,0xb2,0x75,\n",
        "    0x09,0x83,0x2c,0x1a,0x1b,0x6e,0x5a,0xa0,0x52,0x3b,0xd6,0xb3,0x29,0xe3,0x2f,0x84,\n",
        "    0x53,0xd1,0x00,0xed,0x20,0xfc,0xb1,0x5b,0x6a,0xcb,0xbe,0x39,0x4a,0x4c,0x58,0xcf,\n",
        "    0xd0,0xef,0xaa,0xfb,0x43,0x4d,0x33,0x85,0x45,0xf9,0x02,0x7f,0x50,0x3c,0x9f,0xa8,\n",
        "    0x51,0xa3,0x40,0x8f,0x92,0x9d,0x38,0xf5,0xbc,0xb6,0xda,0x21,0x10,0xff,0xf3,0xd2,\n",
        "    0xcd,0x0c,0x13,0xec,0x5f,0x97,0x44,0x17,0xc4,0xa7,0x7e,0x3d,0x64,0x5d,0x19,0x73,\n",
        "    0x60,0x81,0x4f,0xdc,0x22,0x2a,0x90,0x88,0x46,0xee,0xb8,0x14,0xde,0x5e,0x0b,0xdb,\n",
        "    0xe0,0x32,0x3a,0x0a,0x49,0x06,0x24,0x5c,0xc2,0xd3,0xac,0x62,0x91,0x95,0xe4,0x79,\n",
        "    0xe7,0xc8,0x37,0x6d,0x8d,0xd5,0x4e,0xa9,0x6c,0x56,0xf4,0xea,0x65,0x7a,0xae,0x08,\n",
        "    0xba,0x78,0x25,0x2e,0x1c,0xa6,0xb4,0xc6,0xe8,0xdd,0x74,0x1f,0x4b,0xbd,0x8b,0x8a,\n",
        "    0x70,0x3e,0xb5,0x66,0x48,0x03,0xf6,0x0e,0x61,0x35,0x57,0xb9,0x86,0xc1,0x1d,0x9e,\n",
        "    0xe1,0xf8,0x98,0x11,0x69,0xd9,0x8e,0x94,0x9b,0x1e,0x87,0xe9,0xce,0x55,0x28,0xdf,\n",
        "    0x8c,0xa1,0x89,0x0d,0xbf,0xe6,0x42,0x68,0x41,0x99,0x2d,0x0f,0xb0,0x54,0xbb,0x16\n",
        "], dtype=np.uint8)\n",
        "\n",
        "# ---- HELPER FUNCTIONS ----\n",
        "def hamming_weight(x):\n",
        "    \"\"\"Calculate Hamming weight (number of 1s in binary)\"\"\"\n",
        "    return bin(int(x) & 0xFF).count(\"1\")\n",
        "\n",
        "def generate_leakage_kernel(hw, leak_width, amplitude=1.0):\n",
        "    \"\"\"\n",
        "    Generate a Gaussian-shaped leakage kernel\n",
        "    The amplitude is proportional to the Hamming weight\n",
        "    \"\"\"\n",
        "    t = np.linspace(-3, 3, leak_width)\n",
        "    kernel = amplitude * hw * np.exp(-t**2)\n",
        "    return kernel\n",
        "\n",
        "def add_device_variation(trace, device_id, num_devices):\n",
        "    \"\"\"\n",
        "    Add device-specific variations:\n",
        "    - Amplitude scaling\n",
        "    - Baseline shift\n",
        "    - Noise characteristics\n",
        "    \"\"\"\n",
        "    # Device-specific gain (0.8 to 1.2)\n",
        "    gain = 0.8 + 0.4 * (device_id / num_devices)\n",
        "\n",
        "    # Device-specific baseline\n",
        "    baseline = 0.2 * np.sin(2 * np.pi * device_id / num_devices)\n",
        "\n",
        "    # Device-specific noise\n",
        "    device_noise = np.random.normal(0, 0.1 * (1 + device_id / num_devices), len(trace))\n",
        "\n",
        "    return gain * trace + baseline + device_noise\n",
        "\n",
        "def add_environmental_noise(trace):\n",
        "    \"\"\"\n",
        "    Add realistic environmental noise:\n",
        "    - Gaussian noise\n",
        "    - Power line interference (50/60 Hz)\n",
        "    - Random spikes\n",
        "    \"\"\"\n",
        "    # Base Gaussian noise\n",
        "    noise = np.random.normal(0, BASE_NOISE, len(trace))\n",
        "\n",
        "    # Power line interference (simulate 50 Hz)\n",
        "    t = np.arange(len(trace))\n",
        "    powerline = 0.3 * np.sin(2 * np.pi * 50 * t / len(trace))\n",
        "\n",
        "    # Random spikes (electromagnetic interference)\n",
        "    num_spikes = np.random.randint(0, 3)\n",
        "    for _ in range(num_spikes):\n",
        "        spike_pos = np.random.randint(0, len(trace))\n",
        "        spike_width = np.random.randint(5, 15)\n",
        "        spike_amp = np.random.uniform(2, 5) * BASE_NOISE\n",
        "        start = max(0, spike_pos - spike_width // 2)\n",
        "        end = min(len(trace), spike_pos + spike_width // 2)\n",
        "        noise[start:end] += spike_amp\n",
        "\n",
        "    return trace + noise + powerline\n",
        "\n",
        "def add_clock_jitter(trace, max_shift=10):\n",
        "    \"\"\"\n",
        "    Simulate clock jitter by randomly shifting the trace\n",
        "    \"\"\"\n",
        "    shift = np.random.randint(-max_shift, max_shift + 1)\n",
        "    return np.roll(trace, shift)\n",
        "\n",
        "def generate_single_trace(plaintext_byte, key_byte, device_id=0):\n",
        "    \"\"\"\n",
        "    Generate a single power trace for AES first-round S-box operation\n",
        "\n",
        "    The leakage model:\n",
        "    1. Compute intermediate value: SBOX[plaintext ⊕ key]\n",
        "    2. Compute Hamming weight of intermediate value\n",
        "    3. Generate leakage proportional to Hamming weight\n",
        "    4. Add realistic noise and variations\n",
        "\n",
        "    Returns:\n",
        "        trace: Power consumption trace\n",
        "        hw: Hamming weight (label for supervised learning)\n",
        "        intermediate: The intermediate value (for verification)\n",
        "    \"\"\"\n",
        "\n",
        "    trace = np.zeros(TRACE_LENGTH)\n",
        "\n",
        "    intermediate = AES_SBOX[plaintext_byte ^ key_byte]\n",
        "    hw = hamming_weight(intermediate)\n",
        "\n",
        "    signal_amplitude = SNR * BASE_NOISE\n",
        "    kernel = generate_leakage_kernel(hw, LEAK_WIDTH, amplitude=signal_amplitude)\n",
        "\n",
        "    jitter = np.random.randint(-15, 16)\n",
        "    leak_pos = LEAK_CENTER + jitter\n",
        "\n",
        "\n",
        "    start = max(0, leak_pos - LEAK_WIDTH // 2)\n",
        "    end = min(TRACE_LENGTH, leak_pos + LEAK_WIDTH // 2)\n",
        "    kernel_start = max(0, LEAK_WIDTH // 2 - leak_pos)\n",
        "    kernel_end = kernel_start + (end - start)\n",
        "\n",
        "    trace[start:end] += kernel[kernel_start:kernel_end]\n",
        "\n",
        "    trace = add_device_variation(trace, device_id, NUM_DEVICES)\n",
        "\n",
        "    trace = add_environmental_noise(trace)\n",
        "\n",
        "\n",
        "    trace = add_clock_jitter(trace, max_shift=8)\n",
        "\n",
        "    return trace, hw, intermediate\n",
        "\n",
        "# ---- GENERATE DATASET ----\n",
        "def generate_dataset():\n",
        "    \"\"\"Generate complete dataset with metadata\"\"\"\n",
        "    traces = []\n",
        "    labels = []  # Hamming weights (0-8)\n",
        "    metadata = []  # (plaintext, key, device_id, intermediate_value)\n",
        "\n",
        "    # Use a fixed key for the entire dataset (realistic scenario)\n",
        "    fixed_key = np.random.randint(0, 256)\n",
        "\n",
        "    print(f\"Using fixed key byte: 0x{fixed_key:02x}\")\n",
        "    print(f\"Generating {NUM_TRACES} traces...\")\n",
        "\n",
        "    for i in tqdm(range(NUM_TRACES)):\n",
        "        # Random plaintext byte\n",
        "        plaintext = np.random.randint(0, 256)\n",
        "\n",
        "        # Random device ID\n",
        "        device_id = np.random.randint(0, NUM_DEVICES)\n",
        "\n",
        "        # Generate trace\n",
        "        trace, hw, intermediate = generate_single_trace(plaintext, fixed_key, device_id)\n",
        "\n",
        "        traces.append(trace)\n",
        "        labels.append(hw)\n",
        "        metadata.append({\n",
        "            'plaintext': plaintext,\n",
        "            'key': fixed_key,\n",
        "            'device_id': device_id,\n",
        "            'intermediate': intermediate,\n",
        "            'hw': hw\n",
        "        })\n",
        "\n",
        "    return np.array(traces), np.array(labels), metadata, fixed_key\n",
        "\n",
        "# Generate the dataset\n",
        "traces, labels, metadata, true_key = generate_dataset()\n",
        "\n",
        "print(f\"\\nGeneration complete!\")\n",
        "print(f\"Traces shape: {traces.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"Unique labels (Hamming weights): {np.unique(labels)}\")\n",
        "print(f\"Label distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
        "\n",
        "# Calculate SNR to verify\n",
        "signal_power = np.mean([m['hw']**2 for m in metadata]) * (SNR * BASE_NOISE)**2\n",
        "noise_power = BASE_NOISE**2\n",
        "measured_snr = 10 * np.log10(signal_power / noise_power)\n",
        "print(f\"\\nMeasured SNR: {measured_snr:.2f} dB\")\n",
        "\n",
        "# ---- SAVE DATASET ----\n",
        "dataset = {\n",
        "    'traces': traces,\n",
        "    'labels': labels,\n",
        "    'metadata': metadata,\n",
        "    'true_key': true_key,\n",
        "    'config': {\n",
        "        'num_traces': NUM_TRACES,\n",
        "        'trace_length': TRACE_LENGTH,\n",
        "        'num_devices': NUM_DEVICES,\n",
        "        'snr': SNR,\n",
        "        'base_noise': BASE_NOISE,\n",
        "        'leak_center': LEAK_CENTER,\n",
        "        'leak_width': LEAK_WIDTH\n",
        "    }\n",
        "}\n",
        "\n",
        "output_path = os.path.join(OUT_DIR, \"dataset_small.pkl\")\n",
        "with open(output_path, \"wb\") as f:\n",
        "    pickle.dump(dataset, f)\n",
        "\n",
        "print(f\"\\nDataset saved to: {output_path}\")\n",
        "print(f\"File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# ---- GENERATE VISUALIZATION DATA ----\n",
        "# Save a few example traces for visualization\n",
        "example_indices = np.random.choice(len(traces), min(100, len(traces)), replace=False)\n",
        "examples = {\n",
        "    'traces': traces[example_indices],\n",
        "    'labels': labels[example_indices],\n",
        "    'metadata': [metadata[i] for i in example_indices]\n",
        "}\n",
        "\n",
        "example_path = os.path.join(OUT_DIR, \"example_traces.pkl\")\n",
        "with open(example_path, \"wb\") as f:\n",
        "    pickle.dump(examples, f)\n",
        "\n",
        "print(f\"Example traces saved to: {example_path}\")\n",
        "\n",
        "# ---- SUMMARY ----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET GENERATION COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total traces: {NUM_TRACES}\")\n",
        "print(f\"Trace length: {TRACE_LENGTH} samples\")\n",
        "print(f\"Number of classes: {len(np.unique(labels))} (Hamming weights 0-8)\")\n",
        "print(f\"True key byte: 0x{true_key:02x}\")\n",
        "print(f\"\\nYou can now run the training script!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ljexQhepBht",
        "outputId": "053eb62b-db5f-4c58-d8d8-510721ced9b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, backend as K\n",
        "\n",
        "# ---- CONFIG ----\n",
        "DATA_PKL = \"/content/sca_outputs/dataset_small.pkl\"\n",
        "OUT_DIR = \"/content/sca_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE = 0.15  # Additional validation split\n",
        "BATCH = 32  # Reduced for better generalization\n",
        "EPOCHS = 100\n",
        "INITIAL_LR = 1e-4\n",
        "WARMUP_EPOCHS = 5\n",
        "\n",
        "# ---- LOAD DATA ----\n",
        "if not os.path.exists(DATA_PKL):\n",
        "    raise FileNotFoundError(f\"Dataset not found at {DATA_PKL}\")\n",
        "\n",
        "with open(DATA_PKL, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "if isinstance(data, dict):\n",
        "    traces = data.get(\"traces\")\n",
        "    labels = data.get(\"labels\")\n",
        "    if traces is None or labels is None:\n",
        "        traces_candidates = [data.get(\"X\"), data.get(\"traces_all\"), data.get(\"traces_train\")]\n",
        "        labels_candidates = [data.get(\"y\"), data.get(\"labels_all\"), data.get(\"labels_train\")]\n",
        "        traces = next((t for t in traces_candidates if t is not None), None)\n",
        "        labels = next((l for l in labels_candidates if l is not None), None)\n",
        "elif isinstance(data, (list, tuple)) and len(data) >= 2:\n",
        "    traces, labels = data[0], data[1]\n",
        "else:\n",
        "    raise ValueError(\"Unrecognized pickle structure\")\n",
        "\n",
        "traces = np.asarray(traces, dtype=np.float32)\n",
        "labels = np.asarray(labels).squeeze()\n",
        "\n",
        "print(\"Loaded traces shape:\", traces.shape, \"labels:\", labels.shape)\n",
        "print(\"Trace value range: [{:.3f}, {:.3f}]\".format(traces.min(), traces.max()))\n",
        "\n",
        "# ---- ADVANCED PREPROCESSING ----\n",
        "# 1. Trace alignment using correlation-based synchronization\n",
        "def align_traces(traces, reference_idx=0):\n",
        "    \"\"\"Align traces using cross-correlation\"\"\"\n",
        "    reference = traces[reference_idx]\n",
        "    aligned = np.zeros_like(traces)\n",
        "\n",
        "    for i, trace in enumerate(traces):\n",
        "        corr = np.correlate(trace, reference, mode='same')\n",
        "        shift = np.argmax(corr) - len(trace) // 2\n",
        "        aligned[i] = np.roll(trace, -shift)\n",
        "\n",
        "    return aligned\n",
        "\n",
        "print(\"Aligning traces...\")\n",
        "traces = align_traces(traces)\n",
        "\n",
        "# 2. Outlier removal using IQR\n",
        "def remove_outliers(traces, labels, threshold=3.0):\n",
        "    \"\"\"Remove traces with extreme variance (likely corrupted)\"\"\"\n",
        "    trace_vars = np.var(traces, axis=1)\n",
        "    q1, q3 = np.percentile(trace_vars, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - threshold * iqr\n",
        "    upper = q3 + threshold * iqr\n",
        "\n",
        "    mask = (trace_vars >= lower) & (trace_vars <= upper)\n",
        "    return traces[mask], labels[mask]\n",
        "\n",
        "traces, labels = remove_outliers(traces, labels)\n",
        "print(f\"After outlier removal: {traces.shape[0]} traces\")\n",
        "\n",
        "# ---- LABEL HANDLING ----\n",
        "unique = np.unique(labels)\n",
        "print(\"Unique labels:\", len(unique), \"Range:\", unique.min(), \"-\", unique.max())\n",
        "\n",
        "# Map labels to 0-indexed for classification\n",
        "if unique.min() != 0 or not np.array_equal(unique, np.arange(len(unique))):\n",
        "    label_to_idx = {v: i for i, v in enumerate(sorted(unique))}\n",
        "    labels = np.array([label_to_idx[int(v)] for v in labels], dtype=np.int32)\n",
        "    print(\"Remapped labels to 0-indexed\")\n",
        "\n",
        "num_classes = len(np.unique(labels))\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(\"Label distribution:\", dict(Counter(labels.tolist())))\n",
        "\n",
        "# ---- STRATIFIED SPLITS ----\n",
        "# First split: train+val vs test\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    traces, labels, test_size=TEST_SIZE, random_state=SEED, stratify=labels\n",
        ")\n",
        "\n",
        "# Second split: train vs val\n",
        "val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=val_size_adjusted,\n",
        "    random_state=SEED, stratify=y_trainval\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
        "\n",
        "# ---- ROBUST NORMALIZATION ----\n",
        "# Use robust scaling (median/IQR) instead of mean/std\n",
        "def robust_scale(train, val, test):\n",
        "    median = np.median(train, axis=0)\n",
        "    q1 = np.percentile(train, 25, axis=0)\n",
        "    q3 = np.percentile(train, 75, axis=0)\n",
        "    iqr = q3 - q1\n",
        "    iqr[iqr == 0] = 1.0\n",
        "\n",
        "    train_scaled = (train - median) / iqr\n",
        "    val_scaled = (val - median) / iqr\n",
        "    test_scaled = (test - median) / iqr\n",
        "\n",
        "    return train_scaled, val_scaled, test_scaled\n",
        "\n",
        "X_train, X_val, X_test = robust_scale(X_train, X_val, X_test)\n",
        "\n",
        "# Reshape for Conv1D\n",
        "X_train = X_train.reshape(-1, X_train.shape[1], 1).astype(np.float32)\n",
        "X_val = X_val.reshape(-1, X_val.shape[1], 1).astype(np.float32)\n",
        "X_test = X_test.reshape(-1, X_test.shape[1], 1).astype(np.float32)\n",
        "\n",
        "# ---- DATA AUGMENTATION ----\n",
        "class TraceAugmentation(tf.keras.layers.Layer):\n",
        "    \"\"\"Advanced augmentation for power traces\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            # 1. Additive Gaussian noise\n",
        "            noise = tf.random.normal(tf.shape(x), mean=0, stddev=0.1)\n",
        "            x = x + noise\n",
        "\n",
        "            # 2. Random amplitude scaling\n",
        "            scale = tf.random.uniform([], 0.9, 1.1)\n",
        "            x = x * scale\n",
        "\n",
        "            # 3. Random time shift (circular)\n",
        "            shift = tf.random.uniform([], -10, 10, dtype=tf.int32)\n",
        "            x = tf.roll(x, shift, axis=1)\n",
        "\n",
        "            # 4. Random baseline drift\n",
        "            drift = tf.random.uniform([], -0.2, 0.2)\n",
        "            x = x + drift\n",
        "\n",
        "        return x\n",
        "\n",
        "# ---- FOCAL LOSS FOR IMBALANCED CLASSES ----\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"Focal Loss to handle class imbalance\"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
        "\n",
        "        # Clip predictions to prevent log(0)\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Compute focal loss\n",
        "        ce = -y_true_one_hot * tf.math.log(y_pred)\n",
        "        weight = self.alpha * tf.pow(1 - y_pred, self.gamma)\n",
        "        focal_loss = weight * ce\n",
        "\n",
        "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
        "\n",
        "# ---- IMPROVED ARCHITECTURE WITH ATTENTION ----\n",
        "def build_attention_cnn(input_length, num_classes):\n",
        "    \"\"\"\n",
        "    Enhanced CNN with:\n",
        "    - Multi-scale convolutions (inception-like)\n",
        "    - Self-attention mechanism\n",
        "    - Residual connections\n",
        "    - Squeeze-and-Excitation blocks\n",
        "    \"\"\"\n",
        "    inp = layers.Input(shape=(input_length, 1))\n",
        "\n",
        "    # Data augmentation layer (applied during training only)\n",
        "    x = TraceAugmentation()(inp)\n",
        "\n",
        "    # Initial feature extraction\n",
        "    x = layers.Conv1D(64, 11, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    # Multi-scale feature extraction block 1\n",
        "    def inception_block(x, filters):\n",
        "        # Branch 1: 1x1 conv\n",
        "        b1 = layers.Conv1D(filters//4, 1, padding='same', activation='relu')(x)\n",
        "\n",
        "        # Branch 2: 3x3 conv\n",
        "        b2 = layers.Conv1D(filters//4, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "        # Branch 3: 5x5 conv\n",
        "        b3 = layers.Conv1D(filters//4, 5, padding='same', activation='relu')(x)\n",
        "\n",
        "        # Branch 4: max pooling\n",
        "        b4 = layers.MaxPooling1D(3, strides=1, padding='same')(x)\n",
        "        b4 = layers.Conv1D(filters//4, 1, padding='same', activation='relu')(b4)\n",
        "\n",
        "        return layers.Concatenate()([b1, b2, b3, b4])\n",
        "\n",
        "    x = inception_block(x, 64)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Self-attention block\n",
        "    def attention_block(x, filters):\n",
        "        # Multi-head attention\n",
        "        attn = layers.MultiHeadAttention(num_heads=4, key_dim=filters//4)(x, x)\n",
        "        attn = layers.LayerNormalization()(attn)\n",
        "\n",
        "        # Residual connection\n",
        "        x = layers.Add()([x, attn])\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff = layers.Dense(filters * 2, activation='relu')(x)\n",
        "        ff = layers.Dense(filters)(ff)\n",
        "        ff = layers.LayerNormalization()(ff)\n",
        "\n",
        "        return layers.Add()([x, ff])\n",
        "\n",
        "    x = attention_block(x, 64)\n",
        "\n",
        "    # Additional conv blocks\n",
        "    x = layers.Conv1D(128, 9, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = layers.Conv1D(256, 7, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Global pooling with both average and max\n",
        "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
        "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Concatenate()([avg_pool, max_pool])\n",
        "\n",
        "    # Dense layers with residual-like structure\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "model = build_attention_cnn(X_train.shape[1], num_classes)\n",
        "\n",
        "# ---- LEARNING RATE SCHEDULE WITH WARMUP ----\n",
        "def lr_schedule(epoch, lr):\n",
        "    \"\"\"Warmup + cosine annealing\"\"\"\n",
        "    if epoch < WARMUP_EPOCHS:\n",
        "        return INITIAL_LR * (epoch + 1) / WARMUP_EPOCHS\n",
        "    else:\n",
        "        progress = (epoch - WARMUP_EPOCHS) / (EPOCHS - WARMUP_EPOCHS)\n",
        "        return INITIAL_LR * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=INITIAL_LR, weight_decay=1e-4),\n",
        "    loss=FocalLoss(alpha=0.25, gamma=2.0),\n",
        "    metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top3_acc')]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ---- CALLBACKS ----\n",
        "ckpt_path = os.path.join(OUT_DIR, \"best_sca_model.keras\")\n",
        "\n",
        "callback_list = [\n",
        "    callbacks.ModelCheckpoint(\n",
        "        ckpt_path,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        mode=\"max\",\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=15,\n",
        "        mode=\"max\",\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.LearningRateScheduler(lr_schedule, verbose=1),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.TensorBoard(log_dir=os.path.join(OUT_DIR, \"logs\"))\n",
        "]\n",
        "\n",
        "# ---- TRAIN ----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting training with improved architecture...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=callback_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---- EVALUATE ----\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Evaluation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_loss, test_acc, test_top3 = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Top-3 Accuracy: {test_top3:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "y_pred = model.predict(X_test, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(OUT_DIR, \"training_history.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "print(f\"\\nBest model saved to: {ckpt_path}\")\n",
        "print(f\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q6QsFxH-Xi4a",
        "outputId": "7c327139-dc4a-4521-956e-7e699fc81486"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded traces shape: (10000, 600) labels: (10000,)\n",
            "Trace value range: [-5.145, 29.897]\n",
            "Aligning traces...\n",
            "After outlier removal: 9982 traces\n",
            "Unique labels: 9 Range: 0 - 8\n",
            "Number of classes: 9\n",
            "Label distribution: {2: 1109, 3: 2128, 4: 2740, 6: 1107, 5: 2227, 1: 294, 0: 35, 7: 312, 8: 30}\n",
            "Train: 6487, Val: 1498, Test: 1997\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ trace_augmentation  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mTraceAugmentation\u001b[0m) │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m768\u001b[0m │ trace_augmentati… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │      \u001b[38;5;34m1,040\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │      \u001b[38;5;34m3,088\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │      \u001b[38;5;34m5,136\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │      \u001b[38;5;34m1,040\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m16,640\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │      \u001b[38;5;34m8,320\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │      \u001b[38;5;34m8,256\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                     │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m73,856\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m229,632\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m262,656\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │      \u001b[38;5;34m2,313\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ trace_augmentation  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TraceAugmentation</span>) │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ trace_augmentati… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,088</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,136</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                     │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">229,632</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,313</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m749,449\u001b[0m (2.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">749,449</span> (2.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m746,889\u001b[0m (2.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">746,889</span> (2.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,560\u001b[0m (10.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> (10.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Starting training with improved architecture...\n",
            "==================================================\n",
            "\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 2e-05.\n",
            "Epoch 1/100\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.1137 - loss: 0.8168 - top3_acc: 0.3324\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00334, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 206ms/step - accuracy: 0.1137 - loss: 0.8168 - top3_acc: 0.3324 - val_accuracy: 0.0033 - val_loss: 0.7883 - val_top3_acc: 0.4379 - learning_rate: 2.0000e-05\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 4e-05.\n",
            "Epoch 2/100\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1199 - loss: 0.7808 - top3_acc: 0.3457\n",
            "Epoch 2: val_accuracy improved from 0.00334 to 0.06142, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1199 - loss: 0.7808 - top3_acc: 0.3457 - val_accuracy: 0.0614 - val_loss: 0.6503 - val_top3_acc: 0.3932 - learning_rate: 4.0000e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
            "Epoch 3/100\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1223 - loss: 0.7516 - top3_acc: 0.3465\n",
            "Epoch 3: val_accuracy improved from 0.06142 to 0.15287, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1223 - loss: 0.7516 - top3_acc: 0.3466 - val_accuracy: 0.1529 - val_loss: 0.4393 - val_top3_acc: 0.4686 - learning_rate: 6.0000e-05\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 8e-05.\n",
            "Epoch 4/100\n",
            "\u001b[1m202/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1431 - loss: 0.6989 - top3_acc: 0.3884\n",
            "Epoch 4: val_accuracy did not improve from 0.15287\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.1431 - loss: 0.6988 - top3_acc: 0.3884 - val_accuracy: 0.1422 - val_loss: 0.4638 - val_top3_acc: 0.4513 - learning_rate: 8.0000e-05\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5/100\n",
            "\u001b[1m196/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1678 - loss: 0.6450 - top3_acc: 0.4281\n",
            "Epoch 5: val_accuracy improved from 0.15287 to 0.18491, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1675 - loss: 0.6449 - top3_acc: 0.4276 - val_accuracy: 0.1849 - val_loss: 0.4712 - val_top3_acc: 0.4326 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1746 - loss: 0.6146 - top3_acc: 0.4372\n",
            "Epoch 6: val_accuracy improved from 0.18491 to 0.19693, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1747 - loss: 0.6142 - top3_acc: 0.4376 - val_accuracy: 0.1969 - val_loss: 0.4015 - val_top3_acc: 0.5895 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.997266286704631e-05.\n",
            "Epoch 7/100\n",
            "\u001b[1m202/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1812 - loss: 0.5670 - top3_acc: 0.4655\n",
            "Epoch 7: val_accuracy improved from 0.19693 to 0.20828, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1813 - loss: 0.5670 - top3_acc: 0.4656 - val_accuracy: 0.2083 - val_loss: 0.3745 - val_top3_acc: 0.6148 - learning_rate: 9.9973e-05\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.989068136093873e-05.\n",
            "Epoch 8/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1894 - loss: 0.5416 - top3_acc: 0.4982\n",
            "Epoch 8: val_accuracy improved from 0.20828 to 0.23698, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.1899 - loss: 0.5410 - top3_acc: 0.4988 - val_accuracy: 0.2370 - val_loss: 0.3388 - val_top3_acc: 0.6956 - learning_rate: 9.9891e-05\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 9.975414512725057e-05.\n",
            "Epoch 9/100\n",
            "\u001b[1m202/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2504 - loss: 0.4567 - top3_acc: 0.6075\n",
            "Epoch 9: val_accuracy improved from 0.23698 to 0.41989, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.2506 - loss: 0.4564 - top3_acc: 0.6079 - val_accuracy: 0.4199 - val_loss: 0.2167 - val_top3_acc: 0.9019 - learning_rate: 9.9754e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 9.956320346634876e-05.\n",
            "Epoch 10/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3457 - loss: 0.3591 - top3_acc: 0.7489\n",
            "Epoch 10: val_accuracy improved from 0.41989 to 0.48331, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.3462 - loss: 0.3587 - top3_acc: 0.7497 - val_accuracy: 0.4833 - val_loss: 0.1820 - val_top3_acc: 0.9526 - learning_rate: 9.9563e-05\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 9.931806517013612e-05.\n",
            "Epoch 11/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3857 - loss: 0.3089 - top3_acc: 0.8320\n",
            "Epoch 11: val_accuracy improved from 0.48331 to 0.52937, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.3862 - loss: 0.3084 - top3_acc: 0.8326 - val_accuracy: 0.5294 - val_loss: 0.1495 - val_top3_acc: 0.9713 - learning_rate: 9.9318e-05\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 9.901899829374047e-05.\n",
            "Epoch 12/100\n",
            "\u001b[1m201/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4239 - loss: 0.2722 - top3_acc: 0.8733\n",
            "Epoch 12: val_accuracy did not improve from 0.52937\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4240 - loss: 0.2722 - top3_acc: 0.8734 - val_accuracy: 0.5274 - val_loss: 0.1535 - val_top3_acc: 0.9733 - learning_rate: 9.9019e-05\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 9.86663298624003e-05.\n",
            "Epoch 13/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4510 - loss: 0.2506 - top3_acc: 0.8997\n",
            "Epoch 13: val_accuracy improved from 0.52937 to 0.56943, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4512 - loss: 0.2506 - top3_acc: 0.8998 - val_accuracy: 0.5694 - val_loss: 0.1339 - val_top3_acc: 0.9760 - learning_rate: 9.8666e-05\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 9.826044551386744e-05.\n",
            "Epoch 14/100\n",
            "\u001b[1m200/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4745 - loss: 0.2308 - top3_acc: 0.9190\n",
            "Epoch 14: val_accuracy did not improve from 0.56943\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4745 - loss: 0.2308 - top3_acc: 0.9191 - val_accuracy: 0.5461 - val_loss: 0.1527 - val_top3_acc: 0.9720 - learning_rate: 9.8260e-05\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 9.780178907671789e-05.\n",
            "Epoch 15/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4935 - loss: 0.2109 - top3_acc: 0.9406\n",
            "Epoch 15: val_accuracy did not improve from 0.56943\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4936 - loss: 0.2110 - top3_acc: 0.9406 - val_accuracy: 0.5421 - val_loss: 0.1518 - val_top3_acc: 0.9700 - learning_rate: 9.7802e-05\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 9.729086208503174e-05.\n",
            "Epoch 16/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4866 - loss: 0.2174 - top3_acc: 0.9362\n",
            "Epoch 16: val_accuracy did not improve from 0.56943\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4868 - loss: 0.2172 - top3_acc: 0.9364 - val_accuracy: 0.5294 - val_loss: 0.1655 - val_top3_acc: 0.9680 - learning_rate: 9.7291e-05\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 9.672822322997305e-05.\n",
            "Epoch 17/100\n",
            "\u001b[1m201/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5222 - loss: 0.1952 - top3_acc: 0.9511\n",
            "Epoch 17: val_accuracy improved from 0.56943 to 0.57477, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5222 - loss: 0.1952 - top3_acc: 0.9510 - val_accuracy: 0.5748 - val_loss: 0.1374 - val_top3_acc: 0.9726 - learning_rate: 9.6728e-05\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 9.611448774886924e-05.\n",
            "Epoch 18/100\n",
            "\u001b[1m202/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5175 - loss: 0.1886 - top3_acc: 0.9528\n",
            "Epoch 18: val_accuracy improved from 0.57477 to 0.60013, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5176 - loss: 0.1886 - top3_acc: 0.9528 - val_accuracy: 0.6001 - val_loss: 0.1279 - val_top3_acc: 0.9746 - learning_rate: 9.6114e-05\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 9.545032675245813e-05.\n",
            "Epoch 19/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5226 - loss: 0.1805 - top3_acc: 0.9524\n",
            "Epoch 19: val_accuracy did not improve from 0.60013\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5229 - loss: 0.1805 - top3_acc: 0.9525 - val_accuracy: 0.5915 - val_loss: 0.1246 - val_top3_acc: 0.9766 - learning_rate: 9.5450e-05\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 9.473646649103818e-05.\n",
            "Epoch 20/100\n",
            "\u001b[1m201/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5280 - loss: 0.1767 - top3_acc: 0.9581\n",
            "Epoch 20: val_accuracy did not improve from 0.60013\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5281 - loss: 0.1767 - top3_acc: 0.9582 - val_accuracy: 0.6001 - val_loss: 0.1271 - val_top3_acc: 0.9726 - learning_rate: 9.4736e-05\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 9.397368756032445e-05.\n",
            "Epoch 21/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5601 - loss: 0.1647 - top3_acc: 0.9622\n",
            "Epoch 21: val_accuracy did not improve from 0.60013\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5602 - loss: 0.1647 - top3_acc: 0.9622 - val_accuracy: 0.5868 - val_loss: 0.1319 - val_top3_acc: 0.9720 - learning_rate: 9.3974e-05\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 9.316282404787871e-05.\n",
            "Epoch 22/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5463 - loss: 0.1567 - top3_acc: 0.9666\n",
            "Epoch 22: val_accuracy improved from 0.60013 to 0.62684, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5468 - loss: 0.1567 - top3_acc: 0.9665 - val_accuracy: 0.6268 - val_loss: 0.1141 - val_top3_acc: 0.9780 - learning_rate: 9.3163e-05\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 9.230476262104677e-05.\n",
            "Epoch 23/100\n",
            "\u001b[1m196/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5566 - loss: 0.1624 - top3_acc: 0.9672\n",
            "Epoch 23: val_accuracy did not improve from 0.62684\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5567 - loss: 0.1624 - top3_acc: 0.9672 - val_accuracy: 0.6015 - val_loss: 0.1234 - val_top3_acc: 0.9713 - learning_rate: 9.2305e-05\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 9.140044155740101e-05.\n",
            "Epoch 24/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5695 - loss: 0.1511 - top3_acc: 0.9709\n",
            "Epoch 24: val_accuracy improved from 0.62684 to 0.63418, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5694 - loss: 0.1512 - top3_acc: 0.9708 - val_accuracy: 0.6342 - val_loss: 0.1120 - val_top3_acc: 0.9806 - learning_rate: 9.1400e-05\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 9.045084971874738e-05.\n",
            "Epoch 25/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5959 - loss: 0.1435 - top3_acc: 0.9689\n",
            "Epoch 25: val_accuracy did not improve from 0.63418\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5955 - loss: 0.1436 - top3_acc: 0.9689 - val_accuracy: 0.5834 - val_loss: 0.1214 - val_top3_acc: 0.9753 - learning_rate: 9.0451e-05\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 8.945702546981969e-05.\n",
            "Epoch 26/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5917 - loss: 0.1435 - top3_acc: 0.9760\n",
            "Epoch 26: val_accuracy did not improve from 0.63418\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5916 - loss: 0.1436 - top3_acc: 0.9759 - val_accuracy: 0.6242 - val_loss: 0.1115 - val_top3_acc: 0.9773 - learning_rate: 8.9457e-05\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 8.842005554284296e-05.\n",
            "Epoch 27/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5757 - loss: 0.1438 - top3_acc: 0.9693\n",
            "Epoch 27: val_accuracy did not improve from 0.63418\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5760 - loss: 0.1437 - top3_acc: 0.9695 - val_accuracy: 0.5708 - val_loss: 0.1382 - val_top3_acc: 0.9720 - learning_rate: 8.8420e-05\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 8.73410738492077e-05.\n",
            "Epoch 28/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5777 - loss: 0.1393 - top3_acc: 0.9734\n",
            "Epoch 28: val_accuracy did not improve from 0.63418\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5779 - loss: 0.1393 - top3_acc: 0.9734 - val_accuracy: 0.6095 - val_loss: 0.1229 - val_top3_acc: 0.9766 - learning_rate: 8.7341e-05\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 8.622126023955446e-05.\n",
            "Epoch 29/100\n",
            "\u001b[1m196/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6056 - loss: 0.1288 - top3_acc: 0.9781\n",
            "Epoch 29: val_accuracy improved from 0.63418 to 0.65020, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6055 - loss: 0.1290 - top3_acc: 0.9780 - val_accuracy: 0.6502 - val_loss: 0.1137 - val_top3_acc: 0.9806 - learning_rate: 8.6221e-05\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 8.506183921362443e-05.\n",
            "Epoch 30/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5902 - loss: 0.1275 - top3_acc: 0.9765\n",
            "Epoch 30: val_accuracy improved from 0.65020 to 0.66756, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5908 - loss: 0.1275 - top3_acc: 0.9765 - val_accuracy: 0.6676 - val_loss: 0.0952 - val_top3_acc: 0.9873 - learning_rate: 8.5062e-05\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 8.386407858128706e-05.\n",
            "Epoch 31/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6160 - loss: 0.1196 - top3_acc: 0.9800\n",
            "Epoch 31: val_accuracy did not improve from 0.66756\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6158 - loss: 0.1198 - top3_acc: 0.9800 - val_accuracy: 0.6308 - val_loss: 0.1100 - val_top3_acc: 0.9793 - learning_rate: 8.3864e-05\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 8.262928807620843e-05.\n",
            "Epoch 32/100\n",
            "\u001b[1m202/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6096 - loss: 0.1231 - top3_acc: 0.9757\n",
            "Epoch 32: val_accuracy did not improve from 0.66756\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6096 - loss: 0.1231 - top3_acc: 0.9757 - val_accuracy: 0.6442 - val_loss: 0.1021 - val_top3_acc: 0.9826 - learning_rate: 8.2629e-05\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 8.135881792367686e-05.\n",
            "Epoch 33/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6287 - loss: 0.1171 - top3_acc: 0.9765\n",
            "Epoch 33: val_accuracy did not improve from 0.66756\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6284 - loss: 0.1172 - top3_acc: 0.9765 - val_accuracy: 0.6382 - val_loss: 0.0997 - val_top3_acc: 0.9820 - learning_rate: 8.1359e-05\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 8.005405736415126e-05.\n",
            "Epoch 34/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6194 - loss: 0.1214 - top3_acc: 0.9796\n",
            "Epoch 34: val_accuracy improved from 0.66756 to 0.67023, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6194 - loss: 0.1213 - top3_acc: 0.9796 - val_accuracy: 0.6702 - val_loss: 0.0972 - val_top3_acc: 0.9873 - learning_rate: 8.0054e-05\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 7.871643313414718e-05.\n",
            "Epoch 35/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6219 - loss: 0.1186 - top3_acc: 0.9794\n",
            "Epoch 35: val_accuracy improved from 0.67023 to 0.68158, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6221 - loss: 0.1186 - top3_acc: 0.9794 - val_accuracy: 0.6816 - val_loss: 0.0903 - val_top3_acc: 0.9900 - learning_rate: 7.8716e-05\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 7.734740790612136e-05.\n",
            "Epoch 36/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6321 - loss: 0.1091 - top3_acc: 0.9840\n",
            "Epoch 36: val_accuracy did not improve from 0.68158\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6322 - loss: 0.1091 - top3_acc: 0.9839 - val_accuracy: 0.6582 - val_loss: 0.0987 - val_top3_acc: 0.9840 - learning_rate: 7.7347e-05\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 7.594847868906076e-05.\n",
            "Epoch 37/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6359 - loss: 0.1054 - top3_acc: 0.9851\n",
            "Epoch 37: val_accuracy improved from 0.68158 to 0.68358, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6360 - loss: 0.1054 - top3_acc: 0.9851 - val_accuracy: 0.6836 - val_loss: 0.0863 - val_top3_acc: 0.9900 - learning_rate: 7.5948e-05\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 7.452117519152542e-05.\n",
            "Epoch 38/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6318 - loss: 0.1100 - top3_acc: 0.9815\n",
            "Epoch 38: val_accuracy did not improve from 0.68358\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6320 - loss: 0.1100 - top3_acc: 0.9815 - val_accuracy: 0.6555 - val_loss: 0.0948 - val_top3_acc: 0.9833 - learning_rate: 7.4521e-05\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 7.30670581489344e-05.\n",
            "Epoch 39/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6532 - loss: 0.1029 - top3_acc: 0.9855\n",
            "Epoch 39: val_accuracy did not improve from 0.68358\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6532 - loss: 0.1029 - top3_acc: 0.9855 - val_accuracy: 0.6662 - val_loss: 0.0902 - val_top3_acc: 0.9887 - learning_rate: 7.3067e-05\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 7.158771761692464e-05.\n",
            "Epoch 40/100\n",
            "\u001b[1m196/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6526 - loss: 0.1031 - top3_acc: 0.9841\n",
            "Epoch 40: val_accuracy improved from 0.68358 to 0.69693, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6524 - loss: 0.1032 - top3_acc: 0.9840 - val_accuracy: 0.6969 - val_loss: 0.0812 - val_top3_acc: 0.9913 - learning_rate: 7.1588e-05\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 7.008477123264848e-05.\n",
            "Epoch 41/100\n",
            "\u001b[1m196/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6483 - loss: 0.1007 - top3_acc: 0.9854\n",
            "Epoch 41: val_accuracy did not improve from 0.69693\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6484 - loss: 0.1007 - top3_acc: 0.9854 - val_accuracy: 0.6869 - val_loss: 0.0804 - val_top3_acc: 0.9907 - learning_rate: 7.0085e-05\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 6.855986244591104e-05.\n",
            "Epoch 42/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6619 - loss: 0.0954 - top3_acc: 0.9874\n",
            "Epoch 42: val_accuracy improved from 0.69693 to 0.70961, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6619 - loss: 0.0954 - top3_acc: 0.9873 - val_accuracy: 0.7096 - val_loss: 0.0799 - val_top3_acc: 0.9920 - learning_rate: 6.8560e-05\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 6.701465872208216e-05.\n",
            "Epoch 43/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6707 - loss: 0.0972 - top3_acc: 0.9863\n",
            "Epoch 43: val_accuracy did not improve from 0.70961\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6706 - loss: 0.0972 - top3_acc: 0.9864 - val_accuracy: 0.6822 - val_loss: 0.0850 - val_top3_acc: 0.9880 - learning_rate: 6.7015e-05\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 6.545084971874738e-05.\n",
            "Epoch 44/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6799 - loss: 0.0932 - top3_acc: 0.9863\n",
            "Epoch 44: val_accuracy did not improve from 0.70961\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6796 - loss: 0.0933 - top3_acc: 0.9864 - val_accuracy: 0.6796 - val_loss: 0.0843 - val_top3_acc: 0.9873 - learning_rate: 6.5451e-05\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 6.387014543809223e-05.\n",
            "Epoch 45/100\n",
            "\u001b[1m196/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6720 - loss: 0.0943 - top3_acc: 0.9893\n",
            "Epoch 45: val_accuracy improved from 0.70961 to 0.71429, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6718 - loss: 0.0944 - top3_acc: 0.9892 - val_accuracy: 0.7143 - val_loss: 0.0764 - val_top3_acc: 0.9913 - learning_rate: 6.3870e-05\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 6.227427435703997e-05.\n",
            "Epoch 46/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6709 - loss: 0.0955 - top3_acc: 0.9856\n",
            "Epoch 46: val_accuracy did not improve from 0.71429\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6709 - loss: 0.0955 - top3_acc: 0.9856 - val_accuracy: 0.7130 - val_loss: 0.0749 - val_top3_acc: 0.9913 - learning_rate: 6.2274e-05\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 6.066498153718735e-05.\n",
            "Epoch 47/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6651 - loss: 0.0943 - top3_acc: 0.9892\n",
            "Epoch 47: val_accuracy improved from 0.71429 to 0.73298, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6653 - loss: 0.0941 - top3_acc: 0.9891 - val_accuracy: 0.7330 - val_loss: 0.0682 - val_top3_acc: 0.9927 - learning_rate: 6.0665e-05\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 5.90440267166055e-05.\n",
            "Epoch 48/100\n",
            "\u001b[1m200/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6874 - loss: 0.0888 - top3_acc: 0.9891\n",
            "Epoch 48: val_accuracy did not improve from 0.73298\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6873 - loss: 0.0887 - top3_acc: 0.9891 - val_accuracy: 0.7316 - val_loss: 0.0714 - val_top3_acc: 0.9927 - learning_rate: 5.9044e-05\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 5.74131823855921e-05.\n",
            "Epoch 49/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6850 - loss: 0.0873 - top3_acc: 0.9875\n",
            "Epoch 49: val_accuracy improved from 0.73298 to 0.73498, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6848 - loss: 0.0874 - top3_acc: 0.9875 - val_accuracy: 0.7350 - val_loss: 0.0691 - val_top3_acc: 0.9927 - learning_rate: 5.7413e-05\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 5.577423184847932e-05.\n",
            "Epoch 50/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6883 - loss: 0.0880 - top3_acc: 0.9873\n",
            "Epoch 50: val_accuracy did not improve from 0.73498\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6883 - loss: 0.0880 - top3_acc: 0.9873 - val_accuracy: 0.7256 - val_loss: 0.0695 - val_top3_acc: 0.9933 - learning_rate: 5.5774e-05\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 5.4128967273616625e-05.\n",
            "Epoch 51/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6876 - loss: 0.0854 - top3_acc: 0.9880\n",
            "Epoch 51: val_accuracy did not improve from 0.73498\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6876 - loss: 0.0854 - top3_acc: 0.9880 - val_accuracy: 0.7170 - val_loss: 0.0723 - val_top3_acc: 0.9927 - learning_rate: 5.4129e-05\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 5.247918773366112e-05.\n",
            "Epoch 52/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6756 - loss: 0.0836 - top3_acc: 0.9916\n",
            "Epoch 52: val_accuracy did not improve from 0.73498\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.6239593353238888e-05.\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6759 - loss: 0.0836 - top3_acc: 0.9916 - val_accuracy: 0.7316 - val_loss: 0.0694 - val_top3_acc: 0.9920 - learning_rate: 5.2479e-05\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 5.0826697238317935e-05.\n",
            "Epoch 53/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6991 - loss: 0.0845 - top3_acc: 0.9898\n",
            "Epoch 53: val_accuracy improved from 0.73498 to 0.76168, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6991 - loss: 0.0844 - top3_acc: 0.9898 - val_accuracy: 0.7617 - val_loss: 0.0615 - val_top3_acc: 0.9933 - learning_rate: 5.0827e-05\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 4.917330276168208e-05.\n",
            "Epoch 54/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6930 - loss: 0.0810 - top3_acc: 0.9886\n",
            "Epoch 54: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6929 - loss: 0.0810 - top3_acc: 0.9886 - val_accuracy: 0.7143 - val_loss: 0.0728 - val_top3_acc: 0.9927 - learning_rate: 4.9173e-05\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 4.7520812266338885e-05.\n",
            "Epoch 55/100\n",
            "\u001b[1m200/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6941 - loss: 0.0837 - top3_acc: 0.9897\n",
            "Epoch 55: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6942 - loss: 0.0836 - top3_acc: 0.9897 - val_accuracy: 0.7550 - val_loss: 0.0625 - val_top3_acc: 0.9940 - learning_rate: 4.7521e-05\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 4.5871032726383386e-05.\n",
            "Epoch 56/100\n",
            "\u001b[1m195/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7104 - loss: 0.0789 - top3_acc: 0.9902\n",
            "Epoch 56: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7103 - loss: 0.0789 - top3_acc: 0.9901 - val_accuracy: 0.7290 - val_loss: 0.0667 - val_top3_acc: 0.9920 - learning_rate: 4.5871e-05\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 4.4225768151520694e-05.\n",
            "Epoch 57/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7129 - loss: 0.0788 - top3_acc: 0.9890\n",
            "Epoch 57: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7129 - loss: 0.0788 - top3_acc: 0.9891 - val_accuracy: 0.7290 - val_loss: 0.0678 - val_top3_acc: 0.9940 - learning_rate: 4.4226e-05\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 4.2586817614407895e-05.\n",
            "Epoch 58/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7052 - loss: 0.0816 - top3_acc: 0.9893\n",
            "Epoch 58: val_accuracy did not improve from 0.76168\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.1293408281053416e-05.\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7052 - loss: 0.0816 - top3_acc: 0.9893 - val_accuracy: 0.7377 - val_loss: 0.0640 - val_top3_acc: 0.9940 - learning_rate: 4.2587e-05\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 4.095597328339452e-05.\n",
            "Epoch 59/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7068 - loss: 0.0799 - top3_acc: 0.9892\n",
            "Epoch 59: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7069 - loss: 0.0799 - top3_acc: 0.9892 - val_accuracy: 0.7203 - val_loss: 0.0684 - val_top3_acc: 0.9933 - learning_rate: 4.0956e-05\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 3.933501846281267e-05.\n",
            "Epoch 60/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7001 - loss: 0.0770 - top3_acc: 0.9920\n",
            "Epoch 60: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7001 - loss: 0.0770 - top3_acc: 0.9920 - val_accuracy: 0.7557 - val_loss: 0.0610 - val_top3_acc: 0.9940 - learning_rate: 3.9335e-05\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 3.772572564296005e-05.\n",
            "Epoch 61/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7037 - loss: 0.0794 - top3_acc: 0.9900\n",
            "Epoch 61: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7038 - loss: 0.0794 - top3_acc: 0.9900 - val_accuracy: 0.7443 - val_loss: 0.0646 - val_top3_acc: 0.9927 - learning_rate: 3.7726e-05\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 3.612985456190778e-05.\n",
            "Epoch 62/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7065 - loss: 0.0793 - top3_acc: 0.9907\n",
            "Epoch 62: val_accuracy did not improve from 0.76168\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7068 - loss: 0.0792 - top3_acc: 0.9907 - val_accuracy: 0.7557 - val_loss: 0.0618 - val_top3_acc: 0.9927 - learning_rate: 3.6130e-05\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 3.4549150281252636e-05.\n",
            "Epoch 63/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7200 - loss: 0.0728 - top3_acc: 0.9935\n",
            "Epoch 63: val_accuracy improved from 0.76168 to 0.76702, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7200 - loss: 0.0728 - top3_acc: 0.9935 - val_accuracy: 0.7670 - val_loss: 0.0572 - val_top3_acc: 0.9940 - learning_rate: 3.4549e-05\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 3.298534127791785e-05.\n",
            "Epoch 64/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7240 - loss: 0.0747 - top3_acc: 0.9919\n",
            "Epoch 64: val_accuracy did not improve from 0.76702\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7241 - loss: 0.0747 - top3_acc: 0.9919 - val_accuracy: 0.7644 - val_loss: 0.0591 - val_top3_acc: 0.9940 - learning_rate: 3.2985e-05\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 3.144013755408895e-05.\n",
            "Epoch 65/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7204 - loss: 0.0763 - top3_acc: 0.9917\n",
            "Epoch 65: val_accuracy did not improve from 0.76702\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7204 - loss: 0.0763 - top3_acc: 0.9917 - val_accuracy: 0.7423 - val_loss: 0.0635 - val_top3_acc: 0.9947 - learning_rate: 3.1440e-05\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 2.991522876735154e-05.\n",
            "Epoch 66/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7194 - loss: 0.0761 - top3_acc: 0.9897\n",
            "Epoch 66: val_accuracy improved from 0.76702 to 0.77303, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7194 - loss: 0.0760 - top3_acc: 0.9898 - val_accuracy: 0.7730 - val_loss: 0.0564 - val_top3_acc: 0.9947 - learning_rate: 2.9915e-05\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 2.8412282383075363e-05.\n",
            "Epoch 67/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7201 - loss: 0.0729 - top3_acc: 0.9898\n",
            "Epoch 67: val_accuracy did not improve from 0.77303\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7201 - loss: 0.0729 - top3_acc: 0.9898 - val_accuracy: 0.7330 - val_loss: 0.0685 - val_top3_acc: 0.9927 - learning_rate: 2.8412e-05\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 2.693294185106562e-05.\n",
            "Epoch 68/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7315 - loss: 0.0742 - top3_acc: 0.9926\n",
            "Epoch 68: val_accuracy did not improve from 0.77303\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7312 - loss: 0.0742 - top3_acc: 0.9926 - val_accuracy: 0.7664 - val_loss: 0.0577 - val_top3_acc: 0.9940 - learning_rate: 2.6933e-05\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 2.547882480847461e-05.\n",
            "Epoch 69/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7176 - loss: 0.0729 - top3_acc: 0.9922\n",
            "Epoch 69: val_accuracy did not improve from 0.77303\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7177 - loss: 0.0729 - top3_acc: 0.9922 - val_accuracy: 0.7704 - val_loss: 0.0547 - val_top3_acc: 0.9947 - learning_rate: 2.5479e-05\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 2.405152131093926e-05.\n",
            "Epoch 70/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7103 - loss: 0.0722 - top3_acc: 0.9925\n",
            "Epoch 70: val_accuracy did not improve from 0.77303\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7105 - loss: 0.0721 - top3_acc: 0.9925 - val_accuracy: 0.7570 - val_loss: 0.0603 - val_top3_acc: 0.9933 - learning_rate: 2.4052e-05\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 2.2652592093878666e-05.\n",
            "Epoch 71/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7180 - loss: 0.0725 - top3_acc: 0.9919\n",
            "Epoch 71: val_accuracy did not improve from 0.77303\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7180 - loss: 0.0725 - top3_acc: 0.9919 - val_accuracy: 0.7597 - val_loss: 0.0578 - val_top3_acc: 0.9940 - learning_rate: 2.2653e-05\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 2.128356686585282e-05.\n",
            "Epoch 72/100\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7263 - loss: 0.0724 - top3_acc: 0.9912\n",
            "Epoch 72: val_accuracy did not improve from 0.77303\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7263 - loss: 0.0724 - top3_acc: 0.9912 - val_accuracy: 0.7517 - val_loss: 0.0644 - val_top3_acc: 0.9940 - learning_rate: 2.1284e-05\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 1.9945942635848748e-05.\n",
            "Epoch 73/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7198 - loss: 0.0713 - top3_acc: 0.9931\n",
            "Epoch 73: val_accuracy improved from 0.77303 to 0.77704, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7198 - loss: 0.0713 - top3_acc: 0.9932 - val_accuracy: 0.7770 - val_loss: 0.0546 - val_top3_acc: 0.9953 - learning_rate: 1.9946e-05\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 1.8641182076323148e-05.\n",
            "Epoch 74/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7201 - loss: 0.0736 - top3_acc: 0.9922\n",
            "Epoch 74: val_accuracy did not improve from 0.77704\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7202 - loss: 0.0735 - top3_acc: 0.9922 - val_accuracy: 0.7664 - val_loss: 0.0574 - val_top3_acc: 0.9940 - learning_rate: 1.8641e-05\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 1.7370711923791567e-05.\n",
            "Epoch 75/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7211 - loss: 0.0709 - top3_acc: 0.9923\n",
            "Epoch 75: val_accuracy did not improve from 0.77704\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7211 - loss: 0.0709 - top3_acc: 0.9923 - val_accuracy: 0.7577 - val_loss: 0.0588 - val_top3_acc: 0.9947 - learning_rate: 1.7371e-05\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 1.6135921418712956e-05.\n",
            "Epoch 76/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7287 - loss: 0.0699 - top3_acc: 0.9929\n",
            "Epoch 76: val_accuracy improved from 0.77704 to 0.77971, saving model to /content/sca_outputs/best_sca_model.keras\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7288 - loss: 0.0699 - top3_acc: 0.9929 - val_accuracy: 0.7797 - val_loss: 0.0549 - val_top3_acc: 0.9953 - learning_rate: 1.6136e-05\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 1.4938160786375572e-05.\n",
            "Epoch 77/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7258 - loss: 0.0702 - top3_acc: 0.9939\n",
            "Epoch 77: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7258 - loss: 0.0701 - top3_acc: 0.9939 - val_accuracy: 0.7737 - val_loss: 0.0551 - val_top3_acc: 0.9933 - learning_rate: 1.4938e-05\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 1.3778739760445552e-05.\n",
            "Epoch 78/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7259 - loss: 0.0706 - top3_acc: 0.9941\n",
            "Epoch 78: val_accuracy did not improve from 0.77971\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.8893700699845795e-06.\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7260 - loss: 0.0706 - top3_acc: 0.9941 - val_accuracy: 0.7657 - val_loss: 0.0567 - val_top3_acc: 0.9947 - learning_rate: 1.3779e-05\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 1.2658926150792322e-05.\n",
            "Epoch 79/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7226 - loss: 0.0709 - top3_acc: 0.9926\n",
            "Epoch 79: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7226 - loss: 0.0709 - top3_acc: 0.9926 - val_accuracy: 0.7764 - val_loss: 0.0547 - val_top3_acc: 0.9947 - learning_rate: 1.2659e-05\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 1.157994445715706e-05.\n",
            "Epoch 80/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7357 - loss: 0.0708 - top3_acc: 0.9928\n",
            "Epoch 80: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7356 - loss: 0.0707 - top3_acc: 0.9928 - val_accuracy: 0.7737 - val_loss: 0.0554 - val_top3_acc: 0.9940 - learning_rate: 1.1580e-05\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 1.0542974530180327e-05.\n",
            "Epoch 81/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7348 - loss: 0.0680 - top3_acc: 0.9946\n",
            "Epoch 81: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7348 - loss: 0.0680 - top3_acc: 0.9946 - val_accuracy: 0.7777 - val_loss: 0.0553 - val_top3_acc: 0.9940 - learning_rate: 1.0543e-05\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 9.549150281252633e-06.\n",
            "Epoch 82/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7384 - loss: 0.0675 - top3_acc: 0.9937\n",
            "Epoch 82: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7384 - loss: 0.0675 - top3_acc: 0.9937 - val_accuracy: 0.7724 - val_loss: 0.0549 - val_top3_acc: 0.9947 - learning_rate: 9.5492e-06\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 8.599558442598998e-06.\n",
            "Epoch 83/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7269 - loss: 0.0707 - top3_acc: 0.9934\n",
            "Epoch 83: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7269 - loss: 0.0707 - top3_acc: 0.9934 - val_accuracy: 0.7730 - val_loss: 0.0543 - val_top3_acc: 0.9940 - learning_rate: 8.5996e-06\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 7.695237378953223e-06.\n",
            "Epoch 84/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7307 - loss: 0.0700 - top3_acc: 0.9931\n",
            "Epoch 84: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7307 - loss: 0.0699 - top3_acc: 0.9931 - val_accuracy: 0.7623 - val_loss: 0.0571 - val_top3_acc: 0.9933 - learning_rate: 7.6952e-06\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 6.837175952121306e-06.\n",
            "Epoch 85/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7358 - loss: 0.0704 - top3_acc: 0.9924\n",
            "Epoch 85: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7357 - loss: 0.0703 - top3_acc: 0.9924 - val_accuracy: 0.7684 - val_loss: 0.0557 - val_top3_acc: 0.9940 - learning_rate: 6.8372e-06\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 6.026312439675552e-06.\n",
            "Epoch 86/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7371 - loss: 0.0674 - top3_acc: 0.9946\n",
            "Epoch 86: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7369 - loss: 0.0674 - top3_acc: 0.9946 - val_accuracy: 0.7537 - val_loss: 0.0586 - val_top3_acc: 0.9920 - learning_rate: 6.0263e-06\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 5.263533508961827e-06.\n",
            "Epoch 87/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7272 - loss: 0.0680 - top3_acc: 0.9928\n",
            "Epoch 87: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7274 - loss: 0.0680 - top3_acc: 0.9929 - val_accuracy: 0.7637 - val_loss: 0.0568 - val_top3_acc: 0.9940 - learning_rate: 5.2635e-06\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 4.549673247541875e-06.\n",
            "Epoch 88/100\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7324 - loss: 0.0689 - top3_acc: 0.9948\n",
            "Epoch 88: val_accuracy did not improve from 0.77971\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 2.2748365609004395e-06.\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7324 - loss: 0.0689 - top3_acc: 0.9948 - val_accuracy: 0.7724 - val_loss: 0.0546 - val_top3_acc: 0.9947 - learning_rate: 4.5497e-06\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 3.885512251130763e-06.\n",
            "Epoch 89/100\n",
            "\u001b[1m199/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7372 - loss: 0.0670 - top3_acc: 0.9924\n",
            "Epoch 89: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7370 - loss: 0.0670 - top3_acc: 0.9924 - val_accuracy: 0.7437 - val_loss: 0.0609 - val_top3_acc: 0.9907 - learning_rate: 3.8855e-06\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 3.271776770026963e-06.\n",
            "Epoch 90/100\n",
            "\u001b[1m197/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7416 - loss: 0.0666 - top3_acc: 0.9915\n",
            "Epoch 90: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7415 - loss: 0.0666 - top3_acc: 0.9916 - val_accuracy: 0.7684 - val_loss: 0.0554 - val_top3_acc: 0.9940 - learning_rate: 3.2718e-06\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 2.7091379149682685e-06.\n",
            "Epoch 91/100\n",
            "\u001b[1m198/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7411 - loss: 0.0678 - top3_acc: 0.9922\n",
            "Epoch 91: val_accuracy did not improve from 0.77971\n",
            "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7410 - loss: 0.0678 - top3_acc: 0.9923 - val_accuracy: 0.7690 - val_loss: 0.0557 - val_top3_acc: 0.9940 - learning_rate: 2.7091e-06\n",
            "Epoch 91: early stopping\n",
            "Restoring model weights from the end of the best epoch: 76.\n",
            "\n",
            "==================================================\n",
            "Final Evaluation\n",
            "==================================================\n",
            "\n",
            "Test Accuracy: 0.7621\n",
            "Test Top-3 Accuracy: 0.9925\n",
            "Test Loss: 0.0610\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         7\n",
            "           1       0.81      0.51      0.62        59\n",
            "           2       0.84      0.65      0.73       222\n",
            "           3       0.79      0.80      0.79       426\n",
            "           4       0.74      0.92      0.82       548\n",
            "           5       0.75      0.77      0.76       446\n",
            "           6       0.75      0.58      0.65       221\n",
            "           7       0.79      0.48      0.60        62\n",
            "           8       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.76      1997\n",
            "   macro avg       0.61      0.52      0.55      1997\n",
            "weighted avg       0.76      0.76      0.75      1997\n",
            "\n",
            "\n",
            "Best model saved to: /content/sca_outputs/best_sca_model.keras\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from scipy import signal\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"Focal Loss to handle class imbalance\"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
        "\n",
        "        # Clip predictions to prevent log(0)\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Compute focal loss\n",
        "        ce = -y_true_one_hot * tf.math.log(y_pred)\n",
        "        weight = self.alpha * tf.pow(1 - y_pred, self.gamma)\n",
        "        focal_loss = weight * ce\n",
        "\n",
        "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
        "\n",
        "\n",
        "class TraceAugmentation(tf.keras.layers.Layer):\n",
        "    \"\"\"Advanced augmentation for power traces\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            # 1. Additive Gaussian noise\n",
        "            noise = tf.random.normal(tf.shape(x), mean=0, stddev=0.1)\n",
        "            x = x + noise\n",
        "\n",
        "            # 2. Random amplitude scaling\n",
        "            scale = tf.random.uniform([], 0.9, 1.1)\n",
        "            x = x * scale\n",
        "\n",
        "            # 3. Random time shift (circular)\n",
        "            shift = tf.random.uniform([], -10, 10, dtype=tf.int32)\n",
        "            x = tf.roll(x, shift, axis=1)\n",
        "\n",
        "            # 4. Random baseline drift\n",
        "            drift = tf.random.uniform([], -0.2, 0.2)\n",
        "            x = x + drift\n",
        "\n",
        "        return x\n",
        "\n",
        "# Set style for beautiful plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['legend.fontsize'] = 11\n",
        "\n",
        "# Directories\n",
        "DATA_DIR = \"/content/sca_outputs\"\n",
        "OUTPUT_DIR = \"/content/sca_outputs/figures\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Loading data...\")\n",
        "with open(os.path.join(DATA_DIR, \"dataset_small.pkl\"), \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "traces = data['traces']\n",
        "labels = data['labels']\n",
        "metadata = data['metadata']\n",
        "config = data['config']\n",
        "\n",
        "print(f\"Loaded {len(traces)} traces\")\n",
        "\n",
        "print(\"\\nGenerating Figure 1: Power Traces by Hamming Weight...\")\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 10))\n",
        "fig.suptitle('Power Consumption Traces by Hamming Weight (0-8)', fontsize=18, fontweight='bold')\n",
        "\n",
        "for hw in range(9):\n",
        "    ax = axes[hw // 3, hw % 3]\n",
        "\n",
        "    # Find traces with this HW\n",
        "    hw_indices = np.where(labels == hw)[0]\n",
        "\n",
        "    if len(hw_indices) > 0:\n",
        "        # Plot multiple examples with transparency\n",
        "        num_examples = min(20, len(hw_indices))\n",
        "        for i in range(num_examples):\n",
        "            idx = hw_indices[i]\n",
        "            ax.plot(traces[idx], alpha=0.3, color=f'C{hw}', linewidth=0.5)\n",
        "\n",
        "        # Plot mean trace (bold)\n",
        "        mean_trace = np.mean(traces[hw_indices[:50]], axis=0)\n",
        "        ax.plot(mean_trace, color=f'C{hw}', linewidth=2.5, label=f'Mean (n={len(hw_indices)})')\n",
        "\n",
        "        # Mark leakage region\n",
        "        leak_center = config['leak_center']\n",
        "        leak_width = config['leak_width']\n",
        "        ax.axvspan(leak_center - leak_width//2, leak_center + leak_width//2,\n",
        "                   alpha=0.2, color='red', label='Leakage Region')\n",
        "\n",
        "    ax.set_title(f'HW = {hw}', fontweight='bold')\n",
        "    ax.set_xlabel('Time Sample')\n",
        "    ax.set_ylabel('Power Consumption')\n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '1_traces_by_hw.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"Saved: 1_traces_by_hw.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nGenerating Figure 2: Signal vs Noise...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Signal Extraction from Noisy Power Traces', fontsize=18, fontweight='bold')\n",
        "\n",
        "\n",
        "hw4_indices = np.where(labels == 4)[0][:100]\n",
        "hw4_traces = traces[hw4_indices]\n",
        "\n",
        "# 2.1: Single noisy trace\n",
        "axes[0, 0].plot(hw4_traces[0], color='steelblue', linewidth=1, alpha=0.7)\n",
        "axes[0, 0].axvspan(config['leak_center'] - 30, config['leak_center'] + 30,\n",
        "                   alpha=0.2, color='red')\n",
        "axes[0, 0].set_title('(A) Single Noisy Power Trace', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Time Sample')\n",
        "axes[0, 0].set_ylabel('Power')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].text(50, axes[0, 0].get_ylim()[1]*0.9, 'Signal buried in noise',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "# 2.2: Averaging reveals signal\n",
        "mean_trace = np.mean(hw4_traces, axis=0)\n",
        "axes[0, 1].plot(mean_trace, color='darkgreen', linewidth=2.5)\n",
        "axes[0, 1].axvspan(config['leak_center'] - 30, config['leak_center'] + 30,\n",
        "                   alpha=0.2, color='red')\n",
        "axes[0, 1].set_title(f'(B) Average of {len(hw4_traces)} Traces', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Time Sample')\n",
        "axes[0, 1].set_ylabel('Power')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].text(50, axes[0, 1].get_ylim()[1]*0.9, 'Signal clearly visible!',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
        "\n",
        "# 2.3: Signal-to-Noise improvement with averaging\n",
        "num_traces_list = [1, 5, 10, 20, 50, 100]\n",
        "snr_improvements = []\n",
        "\n",
        "for n in num_traces_list:\n",
        "    avg = np.mean(hw4_traces[:n], axis=0)\n",
        "    signal_region = avg[config['leak_center']-20:config['leak_center']+20]\n",
        "    noise_region = avg[:50]  # Baseline region\n",
        "\n",
        "    signal_power = np.var(signal_region)\n",
        "    noise_power = np.var(noise_region)\n",
        "    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else 0\n",
        "    snr_improvements.append(snr)\n",
        "\n",
        "axes[1, 0].plot(num_traces_list, snr_improvements, marker='o', linewidth=2.5,\n",
        "                markersize=10, color='purple')\n",
        "axes[1, 0].set_title('(C) SNR Improvement with Averaging', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Number of Traces Averaged')\n",
        "axes[1, 0].set_ylabel('SNR (dB)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_xscale('log')\n",
        "\n",
        "# 2.4: Overlay different HW\n",
        "axes[1, 1].set_title('(D) Mean Traces for Different Hamming Weights', fontweight='bold')\n",
        "for hw in [1, 3, 5, 7]:\n",
        "    hw_idx = np.where(labels == hw)[0][:50]\n",
        "    if len(hw_idx) > 0:\n",
        "        mean_hw = np.mean(traces[hw_idx], axis=0)\n",
        "        axes[1, 1].plot(mean_hw, label=f'HW={hw}', linewidth=2, alpha=0.8)\n",
        "\n",
        "axes[1, 1].axvspan(config['leak_center'] - 30, config['leak_center'] + 30,\n",
        "                   alpha=0.2, color='red', label='Leakage Region')\n",
        "axes[1, 1].set_xlabel('Time Sample')\n",
        "axes[1, 1].set_ylabel('Power')\n",
        "axes[1, 1].legend(loc='upper right')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '2_signal_vs_noise.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"Saved: 2_signal_vs_noise.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nGenerating Figure 3: Dataset Statistics...\")\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 3.1: Class distribution\n",
        "ax1 = fig.add_subplot(gs[0, :2])\n",
        "hw_counts = Counter(labels)\n",
        "hws = sorted(hw_counts.keys())\n",
        "counts = [hw_counts[hw] for hw in hws]\n",
        "\n",
        "bars = ax1.bar(hws, counts, color=sns.color_palette(\"viridis\", len(hws)),\n",
        "               edgecolor='black', linewidth=1.5)\n",
        "ax1.set_title('Hamming Weight Distribution in Dataset', fontweight='bold', fontsize=14)\n",
        "ax1.set_xlabel('Hamming Weight (# of 1-bits)', fontweight='bold')\n",
        "ax1.set_ylabel('Number of Traces', fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars, counts):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{count}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3.2: Theoretical vs Observed distribution\n",
        "ax2 = fig.add_subplot(gs[0, 2])\n",
        "# Binomial distribution for 8 bits\n",
        "from scipy.stats import binom\n",
        "theoretical = [binom.pmf(k, 8, 0.5) * len(traces) for k in range(9)]\n",
        "observed = [hw_counts.get(k, 0) for k in range(9)]\n",
        "\n",
        "x = np.arange(9)\n",
        "width = 0.35\n",
        "ax2.bar(x - width/2, theoretical, width, label='Theoretical', alpha=0.7, color='lightblue')\n",
        "ax2.bar(x + width/2, observed, width, label='Observed', alpha=0.7, color='salmon')\n",
        "ax2.set_title('Distribution Comparison', fontweight='bold', fontsize=12)\n",
        "ax2.set_xlabel('Hamming Weight')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3.3: Trace statistics\n",
        "ax3 = fig.add_subplot(gs[1, :])\n",
        "trace_means = np.mean(traces, axis=1)\n",
        "trace_stds = np.std(traces, axis=1)\n",
        "trace_vars = np.var(traces, axis=1)\n",
        "\n",
        "ax3.scatter(trace_means, trace_stds, c=labels, cmap='viridis',\n",
        "            alpha=0.5, s=20, edgecolors='none')\n",
        "ax3.set_title('Trace Statistics (Mean vs Std Dev)', fontweight='bold', fontsize=14)\n",
        "ax3.set_xlabel('Mean Power Consumption', fontweight='bold')\n",
        "ax3.set_ylabel('Standard Deviation', fontweight='bold')\n",
        "cbar = plt.colorbar(ax3.collections[0], ax=ax3)\n",
        "cbar.set_label('Hamming Weight', fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 3.4: Device distribution\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "device_counts = Counter([m['device_id'] for m in metadata])\n",
        "devices = sorted(device_counts.keys())\n",
        "dev_counts = [device_counts[d] for d in devices]\n",
        "ax4.bar(devices, dev_counts, color='coral', edgecolor='black')\n",
        "ax4.set_title('Traces per Device', fontweight='bold', fontsize=12)\n",
        "ax4.set_xlabel('Device ID')\n",
        "ax4.set_ylabel('Number of Traces')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "plaintexts = [m['plaintext'] for m in metadata]\n",
        "ax5.hist(plaintexts, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "ax5.set_title('Plaintext Distribution', fontweight='bold', fontsize=12)\n",
        "ax5.set_xlabel('Plaintext Value')\n",
        "ax5.set_ylabel('Frequency')\n",
        "ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3.6: Key info\n",
        "ax6 = fig.add_subplot(gs[2, 2])\n",
        "ax6.axis('off')\n",
        "info_text = f\"\"\"\n",
        "Dataset Configuration:\n",
        "\n",
        "• Total Traces: {len(traces):,}\n",
        "• Trace Length: {traces.shape[1]} samples\n",
        "• Number of Devices: {config['num_devices']}\n",
        "• Signal-to-Noise Ratio: {config['snr']}\n",
        "• Base Noise Level: {config['base_noise']}\n",
        "• Leak Center: {config['leak_center']}\n",
        "• Leak Width: {config['leak_width']}\n",
        "• True Key Byte: 0x{data['true_key']:02X}\n",
        "\"\"\"\n",
        "ax6.text(0.1, 0.5, info_text, fontsize=11, verticalalignment='center',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
        "         family='monospace')\n",
        "\n",
        "plt.suptitle('Dataset Overview and Statistics', fontsize=18, fontweight='bold', y=0.995)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '3_dataset_statistics.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"Saved: 3_dataset_statistics.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nGenerating Figure 4: Training Results...\")\n",
        "\n",
        "\n",
        "history_path = os.path.join(DATA_DIR, \"training_history.pkl\")\n",
        "if os.path.exists(history_path):\n",
        "    with open(history_path, \"rb\") as f:\n",
        "        history = pickle.load(f)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle('Model Training Performance', fontsize=18, fontweight='bold')\n",
        "\n",
        "    epochs = range(1, len(history['loss']) + 1)\n",
        "\n",
        "    # 4.1: Loss curves\n",
        "    axes[0, 0].plot(epochs, history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
        "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
        "    axes[0, 0].set_title('Loss During Training', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4.2: Accuracy curves\n",
        "    axes[0, 1].plot(epochs, history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\n",
        "    axes[0, 1].plot(epochs, history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\n",
        "    axes[0, 1].set_title('Accuracy During Training', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].set_ylim([0, 1])\n",
        "\n",
        "    # 4.3: Top-3 accuracy\n",
        "    if 'top3_acc' in history:\n",
        "        axes[1, 0].plot(epochs, history['top3_acc'], 'b-', linewidth=2, label='Training Top-3')\n",
        "        axes[1, 0].plot(epochs, history['val_top3_acc'], 'r-', linewidth=2, label='Validation Top-3')\n",
        "        axes[1, 0].set_title('Top-3 Accuracy During Training', fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Top-3 Accuracy')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].set_ylim([0, 1])\n",
        "\n",
        "    # 4.4: Learning rate schedule\n",
        "    if 'lr' in history:\n",
        "        axes[1, 1].plot(epochs, history['lr'], 'g-', linewidth=2)\n",
        "        axes[1, 1].set_title('Learning Rate Schedule', fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Learning Rate')\n",
        "        axes[1, 1].set_yscale('log')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, '4_training_results.png'), dpi=300, bbox_inches='tight')\n",
        "    print(f\"Saved: 4_training_results.png\")\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"Training history not found, skipping Figure 4\")\n",
        "\n",
        "\n",
        "print(\"\\nGenerating Figure 5: Attack Workflow...\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
        "ax.axis('off')\n",
        "\n",
        "# Draw workflow boxes\n",
        "boxes = [\n",
        "    (0.1, 0.85, \"1. Data Collection\", \"Measure power traces\\nduring AES encryption\"),\n",
        "    (0.4, 0.85, \"2. Preprocessing\", \"Align traces, remove\\noutliers, normalize\"),\n",
        "    (0.7, 0.85, \"3. Feature Extraction\", \"CNN extracts temporal\\npatterns from traces\"),\n",
        "    (0.1, 0.55, \"4. Model Training\", \"Learn relationship between\\ntraces and HW\"),\n",
        "    (0.4, 0.55, \"5. Key Recovery\", \"Predict HW for unknown\\nkey hypotheses\"),\n",
        "    (0.7, 0.55, \"6. Attack Success\", \"Recover secret AES key\\nfrom predictions\"),\n",
        "]\n",
        "\n",
        "for x, y, title, desc in boxes:\n",
        "    bbox = dict(boxstyle='round,pad=0.5', facecolor='lightblue',\n",
        "                edgecolor='black', linewidth=2)\n",
        "    ax.text(x, y, f\"{title}\\n\\n{desc}\", transform=ax.transAxes,\n",
        "            fontsize=12, verticalalignment='top', bbox=bbox,\n",
        "            ha='left', fontweight='bold')\n",
        "\n",
        "# Draw arrows\n",
        "arrow_props = dict(arrowstyle='->', lw=2.5, color='red')\n",
        "ax.annotate('', xy=(0.35, 0.88), xytext=(0.25, 0.88),\n",
        "            arrowprops=arrow_props, transform=ax.transAxes)\n",
        "ax.annotate('', xy=(0.65, 0.88), xytext=(0.55, 0.88),\n",
        "            arrowprops=arrow_props, transform=ax.transAxes)\n",
        "ax.annotate('', xy=(0.2, 0.82), xytext=(0.2, 0.68),\n",
        "            arrowprops=arrow_props, transform=ax.transAxes)\n",
        "ax.annotate('', xy=(0.35, 0.58), xytext=(0.25, 0.58),\n",
        "            arrowprops=arrow_props, transform=ax.transAxes)\n",
        "ax.annotate('', xy=(0.65, 0.58), xytext=(0.55, 0.58),\n",
        "            arrowprops=arrow_props, transform=ax.transAxes)\n",
        "\n",
        "# Add example trace\n",
        "ax_trace = fig.add_axes([0.15, 0.15, 0.7, 0.25])\n",
        "example_trace = traces[np.where(labels == 4)[0][0]]\n",
        "ax_trace.plot(example_trace, color='darkblue', linewidth=1.5)\n",
        "ax_trace.set_title('Example Power Trace', fontweight='bold', fontsize=14)\n",
        "ax_trace.set_xlabel('Time Sample')\n",
        "ax_trace.set_ylabel('Power')\n",
        "ax_trace.grid(True, alpha=0.3)\n",
        "ax_trace.axvspan(config['leak_center']-30, config['leak_center']+30,\n",
        "                 alpha=0.2, color='red')\n",
        "\n",
        "plt.suptitle('Side-Channel Attack Workflow', fontsize=18, fontweight='bold', y=0.98)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, '5_attack_workflow.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"Saved: 5_attack_workflow.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "print(\"\\nGenerating Figure 6: Model Performance Analysis...\")\n",
        "\n",
        "# Try to load model and make predictions\n",
        "model_path = os.path.join(DATA_DIR, \"best_sca_model.keras\")\n",
        "if os.path.exists(model_path):\n",
        "    # Load test data (use 20% as test)\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "    _, X_test, _, y_test = train_test_split(traces, labels, test_size=0.2,\n",
        "                                             random_state=42, stratify=labels)\n",
        "\n",
        "    # Normalize\n",
        "    median = np.median(traces, axis=0)\n",
        "    q1, q3 = np.percentile(traces, [25, 75], axis=0)\n",
        "    iqr = q3 - q1\n",
        "    iqr[iqr == 0] = 1.0\n",
        "    X_test_scaled = (X_test - median) / iqr\n",
        "    X_test_scaled = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
        "\n",
        "    # Load model and predict, providing custom objects\n",
        "    model = tf.keras.models.load_model(model_path, compile=False, custom_objects={'TraceAugmentation': TraceAugmentation, 'FocalLoss': FocalLoss})\n",
        "    y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # 6.1: Confusion matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "                xticklabels=range(9), yticklabels=range(9),\n",
        "                cbar_kws={'label': 'Number of Samples'})\n",
        "    axes[0].set_title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
        "    axes[0].set_xlabel('Predicted Hamming Weight', fontweight='bold')\n",
        "    axes[0].set_ylabel('True Hamming Weight', fontweight='bold')\n",
        "\n",
        "    # 6.2: Per-class accuracy\n",
        "    class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "    class_support = cm.sum(axis=1)\n",
        "\n",
        "    x_pos = np.arange(9)\n",
        "    bars = axes[1].bar(x_pos, class_acc, color=sns.color_palette(\"viridis\", 9),\n",
        "                       edgecolor='black', linewidth=1.5)\n",
        "    axes[1].set_title('Per-Class Accuracy', fontweight='bold', fontsize=14)\n",
        "    axes[1].set_xlabel('Hamming Weight', fontweight='bold')\n",
        "    axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
        "    axes[1].set_ylim([0, 1])\n",
        "    axes[1].set_xticks(x_pos)\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add support counts\n",
        "    for i, (bar, support) in enumerate(zip(bars, class_support)):\n",
        "        height = bar.get_height()\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                    f'{height:.2f}\\n(n={support})', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, '6_model_performance.png'), dpi=300, bbox_inches='tight')\n",
        "    print(f\"Saved: 6_model_performance.png\")\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"Model not found, skipping Figure 6\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VISUALIZATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nAll figures saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\nGenerated figures:\")\n",
        "print(\"1. 1_traces_by_hw.png - Power traces for each Hamming weight\")\n",
        "print(\"2. 2_signal_vs_noise.png - Signal extraction demonstration\")\n",
        "print(\"3. 3_dataset_statistics.png - Dataset overview\")\n",
        "print(\"4. 4_training_results.png - Model training curves\")\n",
        "print(\"5. 5_attack_workflow.png - Attack methodology\")\n",
        "print(\"6. 6_model_performance.png - Confusion matrix and accuracy\")\n",
        "print(\"\\nThese figures are ready for your presentation!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sBQ4yXJpVCq",
        "outputId": "7a2cfca8-7209-46be-e055-cdc3da244d4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded 10000 traces\n",
            "\n",
            "Generating Figure 1: Power Traces by Hamming Weight...\n",
            "Saved: 1_traces_by_hw.png\n",
            "\n",
            "Generating Figure 2: Signal vs Noise...\n",
            "Saved: 2_signal_vs_noise.png\n",
            "\n",
            "Generating Figure 3: Dataset Statistics...\n",
            "Saved: 3_dataset_statistics.png\n",
            "\n",
            "Generating Figure 4: Training Results...\n",
            "Saved: 4_training_results.png\n",
            "\n",
            "Generating Figure 5: Attack Workflow...\n",
            "Saved: 5_attack_workflow.png\n",
            "\n",
            "Generating Figure 6: Model Performance Analysis...\n",
            "Saved: 6_model_performance.png\n",
            "\n",
            "============================================================\n",
            "VISUALIZATION COMPLETE!\n",
            "============================================================\n",
            "\n",
            "All figures saved to: /content/sca_outputs/figures\n",
            "\n",
            "Generated figures:\n",
            "1. 1_traces_by_hw.png - Power traces for each Hamming weight\n",
            "2. 2_signal_vs_noise.png - Signal extraction demonstration\n",
            "3. 3_dataset_statistics.png - Dataset overview\n",
            "4. 4_training_results.png - Model training curves\n",
            "5. 5_attack_workflow.png - Attack methodology\n",
            "6. 6_model_performance.png - Confusion matrix and accuracy\n",
            "\n",
            "These figures are ready for your presentation!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "%%bash\n",
        "GITHUB_USERNAME=\"ahmedElmersawy\"\n",
        "REPO_NAME=\"machine-learning-side-channel-leakage-detection\"\n",
        "TOKEN=\"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\"\n",
        "\n",
        "cd /content/$REPO_NAME\n",
        "\n",
        "git remote remove origin 2>/dev/null\n",
        "git remote add origin https://$GITHUB_USERNAME:$TOKEN@github.com/$GITHUB_USERNAME/$REPO_NAME.git\n",
        "\n",
        "git push -u origin main --force\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hVunmytaKuC",
        "outputId": "13f447d2-4d55-4e4b-c474-93705b241781"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "To https://github.com/ahmedElmersawy/machine-learning-side-channel-leakage-detection.git\n",
            " + 40f00d6...1624ebb main -> main (forced update)\n"
          ]
        }
      ]
    }
  ]
}